# -*- coding: utf-8 -*-
"""Credit_Card_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQoaoKQyvFEaDj_88AfKcxdUg7pYCpyK
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

df = pd.read_csv('card_transdata.csv')

"""# Data Prepartion"""

df.head()

df[df['distance_from_home']<0]

# Check data quality
print(df.info())  # Check data types and missing values
print(df.duplicated().sum())  # Check for duplicates

# Calculate the percentage of fraudulent transactions
fraudulent_percentage = (df['fraud'].sum() / len(df)) * 100

print(f"The percentage of fraudulent transactions is: {fraudulent_percentage:.2f}%")

print(len(df[df['fraud']==1])) #theft
print(len(df[df['fraud']==0]))

# Calculate the percentage of fraudulent transactions
fraudulent_percentage = (df['fraud'].sum() / len(df)) * 100
print(f"The percentage of fraudulent transactions is: {fraudulent_percentage:.2f}%")

# Explore distance-related features
fraudulent_distances = df.loc[df['fraud'] == 1, ['distance_from_home', 'distance_from_last_transaction']]
non_fraudulent_distances = df.loc[df['fraud'] == 0, ['distance_from_home', 'distance_from_last_transaction']]

# Plot distance-related features
plt.scatter(non_fraudulent_distances['distance_from_home'], non_fraudulent_distances['distance_from_last_transaction'],
            color='blue', label='Non-Fraudulent')
plt.scatter(fraudulent_distances['distance_from_home'], fraudulent_distances['distance_from_last_transaction'],
            color='red', label='Fraudulent')
plt.xlabel('Distance from Home')
plt.ylabel('Distance from Last Transaction')
plt.title('Fraudulent vs Non-Fraudulent Transactions - Distance')
plt.legend()
plt.show()

# Explore ratio to median purchase price
fraudulent_ratios = df.loc[df['fraud'] == 1, 'ratio_to_median_purchase_price']
non_fraudulent_ratios = df.loc[df['fraud'] == 0, 'ratio_to_median_purchase_price']

# Plot ratio to median purchase price
plt.hist(non_fraudulent_ratios, bins=30, alpha=0.5, color='blue', label='Non-Fraudulent')
plt.hist(fraudulent_ratios, bins=30, alpha=0.5, color='red', label='Fraudulent')
plt.xlabel('Ratio to Median Purchase Price')
plt.ylabel('Frequency')
plt.title('Fraudulent vs Non-Fraudulent Transactions - Ratio to Median Purchase Price')
plt.legend()
plt.show()

# Explore other features (repeat retailer, chip, pin number, online order)
fraudulent_repeat_retailer = df.loc[df['fraud'] == 1, 'repeat_retailer'].value_counts(normalize=True)
non_fraudulent_repeat_retailer = df.loc[df['fraud'] == 0, 'repeat_retailer'].value_counts(normalize=True)

fraudulent_chip = df.loc[df['fraud'] == 1, 'used_chip'].value_counts(normalize=True)
non_fraudulent_chip = df.loc[df['fraud'] == 0, 'used_chip'].value_counts(normalize=True)

fraudulent_pin = df.loc[df['fraud'] == 1, 'used_pin_number'].value_counts(normalize=True)
non_fraudulent_pin = df.loc[df['fraud'] == 0, 'used_pin_number'].value_counts(normalize=True)

fraudulent_online_order = df.loc[df['fraud'] == 1, 'online_order'].value_counts(normalize=True)
non_fraudulent_online_order = df.loc[df['fraud'] == 0, 'online_order'].value_counts(normalize=True)

print("Percentage of repeat retailers for fraudulent transactions:")
print(fraudulent_repeat_retailer)
print("\nPercentage of repeat retailers for non-fraudulent transactions:")
print(non_fraudulent_repeat_retailer)

print("\nPercentage of chip-enabled cards for fraudulent transactions:")
print(fraudulent_chip)
print("\nPercentage of chip-enabled cards for non-fraudulent transactions:")
print(non_fraudulent_chip)

print("\nPercentage of PIN numbers for fraudulent transactions:")
print(fraudulent_pin)
print("\nPercentage of PIN numbers for non-fraudulent transactions:")
print(non_fraudulent_pin)

print("\nPercentage of online orders for fraudulent transactions:")
print(fraudulent_online_order)
print("\nPercentage of online orders for non-fraudulent transactions:")
print(non_fraudulent_online_order)

for label in df.columns[:-1]:
    plt.hist(df[df['fraud']==1][label], color = 'blue', label='theft', alpha=0.7, density = True)
    plt.hist(df[df['fraud']==0][label], color = 'red', label='non-theft', alpha=0.7, density = True)
    plt.title(label)
    plt.ylabel('Probability')
    plt.xlabel(label)
    plt.legend()
    plt.show()

"""# Train, validation, test datasets"""

train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])

print(len(train[train['fraud']==1])) #theft
print(len(train[train['fraud']==0]))

"""## Oversampling Method"""

def scale_dataset(dataframe, oversample=False):
    x = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    if oversample:
        ros = RandomOverSampler()
        x,y = ros.fit_resample(x, y)

    data = np.hstack((x, np.reshape(y, (-1, 1))))

    return data,x,y

print(len(train[train['fraud']==1])) #theft
print(len(train[train['fraud']==0]))

train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)

len(y_train)

len(x_train)

x_train.shape

sum(y_train == 1)

sum(y_train == 0)

"""## Undersampling Method"""

def scale_dataset(dataframe, undersample=False):
    x = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    if undersample:
        rus = RandomUnderSampler()
        x, y = rus.fit_resample(x, y)

    data = np.hstack((x, np.reshape(y, (-1, 1))))

    return data, x, y

"""# Classification Models

## kNN
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, f1_score

knn_model = KNeighborsClassifier(n_neighbors=1)
knn_model.fit(x_train, y_train)

y_pred = knn_model.predict(x_test)

np.unique(y_pred, return_counts=True)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)

f1_score(y_test, y_pred)

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model = nb_model.fit(x_train, y_train)

y_pred = nb_model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)

"""## Log Regression"""

from sklearn.linear_model import LogisticRegression

lg_model = LogisticRegression()
lg_model = lg_model.fit(x_train, y_train)

y_pred = lg_model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)

import joblib
joblib.dump(lg_model, 'lg_model.jl')

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(x_train, y_train)

y_pred = dt_model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)

### Create a Pickle file using serialization
import pickle
pickle_out = open("dt_model.pkl","wb")
pickle.dump(dt_model, pickle_out)
pickle_out.close()

import joblib
joblib.dump(dt_model, 'dt_model.jl')

pickle_in = open("dt_model.pkl","rb")
dt_model=pickle.load(pickle_in)

dt_model.predict([[2,3,4,1,16,5,1]])

"""## SVM"""

from sklearn.svm import SVC

svm_model = SVC()
svm_model = svm_model.fit(x_train, y_train)

y_pred = svm_model.predict(x_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(cm)
plt